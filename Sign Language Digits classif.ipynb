{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Digits\n",
    "Kaggel data =https://www.kaggle.com/ardamavi/sign-language-digits-dataset\n",
    "\n",
    "the scope of thos project to detect sign language digits (0-9) \n",
    "\n",
    "avilable data \n",
    "\n",
    "Image size: 100x100\n",
    "Color space: RGB\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "##### read data \n",
    "##### preprocessing \n",
    "##### convert color space to grey scale then thershold\n",
    "##### save data as npy \n",
    "##### use keras (TF-based) to create model (CNN)\n",
    "##### train\n",
    "##### Predicte and Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/anaconda3/envs/newenvt/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "path ='./Dataset'\n",
    "\n",
    "image_list =[]\n",
    "lable =[]\n",
    "\n",
    "for r, d, f in os.walk(path):\n",
    "    for folder in d:\n",
    "        folder_path=os.path.join(r, folder)\n",
    "        \n",
    "        for r1, d1, f1 in os.walk(folder_path):\n",
    "            for file in f1:\n",
    "                if '.JPG' in file:\n",
    "                    file_path =str(folder_path)+\"/\"+str(file)\n",
    "                    sample = cv2.imread(file_path,0)\n",
    "                    if sample.shape != (100,100):\n",
    "                        sample = cv2.resize(sample,(100,100))\n",
    "                    #thershold for the images\n",
    "                    ret,thresh3 = cv2.threshold(sample,150,255,cv2.THRESH_TRUNC)\n",
    "                    #rescaling the vakues to 0-1 range\n",
    "                    X = scale( thresh3, axis=0, with_mean=True, with_std=True, copy=True )\n",
    "                    image_list.append(X)\n",
    "                    lable.append(int(folder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create numpy and save it to npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2062\n",
      "2062\n"
     ]
    }
   ],
   "source": [
    "print (len(image_list))\n",
    "print (len(lable))\n",
    "procesed_data = np.array(image_list)\n",
    "lable_data = np.array(lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('npy_dataset/'):\n",
    "            os.makedirs('npy_dataset/')\n",
    "np.save('npy_dataset/procesed_data.npy', procesed_data)\n",
    "np.save('npy_dataset/lable_data.npy', lable_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorizre the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/anaconda3/envs/newenvt/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "lable_data = to_categorical(lable_data, 10)\n",
    "lable_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KerasModel(input_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "procesed_data =procesed_data.reshape(2062,100,100,1) #adjust theshape\n",
    "#split data to test & train\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(procesed_data, lable_data, test_size=0.3, random_state=101)\n",
    "\n",
    "model = KerasModel((100,100,1))\n",
    "\n",
    "model.compile(optimizer = \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1443/1443 [==============================] - 66s 46ms/step - loss: 0.3585 - acc: 0.8972\n",
      "Epoch 2/20\n",
      "1443/1443 [==============================] - 69s 48ms/step - loss: 0.1759 - acc: 0.9307\n",
      "Epoch 3/20\n",
      "1443/1443 [==============================] - 62s 43ms/step - loss: 0.1163 - acc: 0.9561\n",
      "Epoch 4/20\n",
      "1443/1443 [==============================] - 65s 45ms/step - loss: 0.0847 - acc: 0.9676\n",
      "Epoch 5/20\n",
      "1443/1443 [==============================] - 61s 43ms/step - loss: 0.0629 - acc: 0.9753\n",
      "Epoch 6/20\n",
      "1443/1443 [==============================] - 60s 42ms/step - loss: 0.0475 - acc: 0.9813\n",
      "Epoch 7/20\n",
      "1443/1443 [==============================] - 64s 45ms/step - loss: 0.0344 - acc: 0.9875\n",
      "Epoch 8/20\n",
      "1443/1443 [==============================] - 64s 45ms/step - loss: 0.0370 - acc: 0.9851\n",
      "Epoch 9/20\n",
      "1443/1443 [==============================] - 61s 42ms/step - loss: 0.0317 - acc: 0.9889\n",
      "Epoch 10/20\n",
      "1443/1443 [==============================] - 62s 43ms/step - loss: 0.0320 - acc: 0.9882\n",
      "Epoch 11/20\n",
      "1443/1443 [==============================] - 63s 44ms/step - loss: 0.0224 - acc: 0.9911\n",
      "Epoch 12/20\n",
      "1443/1443 [==============================] - 67s 47ms/step - loss: 0.0201 - acc: 0.9923\n",
      "Epoch 13/20\n",
      "1443/1443 [==============================] - 66s 46ms/step - loss: 0.0217 - acc: 0.9922\n",
      "Epoch 14/20\n",
      "1443/1443 [==============================] - 63s 43ms/step - loss: 0.0179 - acc: 0.9933\n",
      "Epoch 15/20\n",
      "1443/1443 [==============================] - 66s 46ms/step - loss: 0.0193 - acc: 0.9935\n",
      "Epoch 16/20\n",
      "1443/1443 [==============================] - 68s 47ms/step - loss: 0.0140 - acc: 0.9945\n",
      "Epoch 17/20\n",
      "1443/1443 [==============================] - 63s 43ms/step - loss: 0.0148 - acc: 0.9935\n",
      "Epoch 18/20\n",
      "1443/1443 [==============================] - 65s 45ms/step - loss: 0.0134 - acc: 0.9944\n",
      "Epoch 19/20\n",
      "1443/1443 [==============================] - 63s 44ms/step - loss: 0.0125 - acc: 0.9951\n",
      "Epoch 20/20\n",
      "1443/1443 [==============================] - 65s 45ms/step - loss: 0.0143 - acc: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ca0b53198>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train,y = Y_train,epochs =20 ,batch_size = 16 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 5s 8ms/step\n",
      "\n",
      "Loss = 0.14370174718018688\n",
      "Test Accuracy = 0.9657512262872809\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(x = X_test,y = Y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
